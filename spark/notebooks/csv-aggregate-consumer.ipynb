{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('csv-changes-event-consumer')\n",
    "         # Add kafka package\n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stream dataframe setting kafka server, topic and offset option\n",
    "df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka-server:29092\") # kafka server ip address inspect - something like 172.23.0.5\n",
    "  .option(\"subscribe\", \"national-health-indicators\") # topic\n",
    "  .option(\"startingOffsets\", \"earliest\") # start from beginning \n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Convert binary to string key and value\n",
    "df1 = (df\n",
    "    .withColumn(\"key\", df[\"key\"].cast(StringType()))\n",
    "    .withColumn(\"value\", df[\"value\"].cast(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print to jupyter output in two fases - memory and display\n",
    "query = df1 \\\n",
    "    .withWatermark(\"timestampstr\", \"3 minutes\") \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"csv_query\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "|key|               value|               topic|partition|offset|           timestamp|timestampType|\n",
      "+---+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "|  1|{\"id\":1,\"nome\":\"j...|national-health-i...|        0|     0|2022-03-20 00:22:...|            0|\n",
      "|  3|{\"id\":3,\"nome\":\"a...|national-health-i...|        0|     1|2022-03-20 00:22:...|            0|\n",
      "|  3|{\"id\":3,\"nome\":\"a...|national-health-i...|        0|     2|2022-03-20 00:22:...|            0|\n",
      "|  2|{\"id\":2,\"nome\":\"m...|national-health-i...|        0|     3|2022-03-20 00:22:...|            0|\n",
      "|  3|{\"id\":3,\"nome\":\"a...|national-health-i...|        0|     4|2022-03-20 00:22:...|            0|\n",
      "|  1|{\"id\":1,\"nome\":\"j...|national-health-i...|        0|     5|2022-03-20 00:22:...|            0|\n",
      "|  1|{\"id\":1,\"nome\":\"j...|national-health-i...|        0|     6|2022-03-20 00:22:...|            0|\n",
      "|  1|{\"id\":1,\"nome\":\"j...|national-health-i...|        0|     7|2022-03-20 00:22:...|            0|\n",
      "|  2|{\"id\":2,\"nome\":\"m...|national-health-i...|        0|     8|2022-03-20 00:22:...|            0|\n",
      "|  3|{\"id\":3,\"nome\":\"a...|national-health-i...|        0|     9|2022-03-20 00:22:...|            0|\n",
      "|  3|{\"id\":3,\"nome\":\"a...|national-health-i...|        0|    10|2022-03-20 00:22:...|            0|\n",
      "|  2|{\"id\":2,\"nome\":\"m...|national-health-i...|        0|    11|2022-03-20 00:22:...|            0|\n",
      "|  2|{\"id\":2,\"nome\":\"m...|national-health-i...|        0|    12|2022-03-20 00:22:...|            0|\n",
      "|  3|{\"id\":3,\"nome\":\"a...|national-health-i...|        0|    13|2022-03-20 00:22:...|            0|\n",
      "|  1|{\"id\":1,\"nome\":\"j...|national-health-i...|        0|    14|2022-03-20 00:22:...|            0|\n",
      "|  2|{\"id\":2,\"nome\":\"m...|national-health-i...|        0|    15|2022-03-20 00:22:...|            0|\n",
      "|  2|{\"id\":2,\"nome\":\"m...|national-health-i...|        0|    16|2022-03-20 00:22:...|            0|\n",
      "|  3|{\"id\":3,\"nome\":\"a...|national-health-i...|        0|    17|2022-03-20 00:22:...|            0|\n",
      "|  3|{\"id\":3,\"nome\":\"a...|national-health-i...|        0|    18|2022-03-20 00:22:...|            0|\n",
      "|  2|{\"id\":2,\"nome\":\"m...|national-health-i...|        0|    19|2022-03-20 00:22:...|            0|\n",
      "+---+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a579930eb018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SELECT * FROM csv_query'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m199\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# display the output\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(query.status)\n",
    "    display(spark.sql('SELECT * FROM csv_query').show())\n",
    "    time.sleep(199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative - print to console\n",
    "df1.writeStream \\\n",
    "   .format(\"console\") \\\n",
    "   .outputMode(\"append\") \\\n",
    "   .start() \\\n",
    "   .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, LongType, IntegerType, DoubleType\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Event data schema\n",
    "schema_csv = StructType(\n",
    "    [StructField(\"$schema\",StringType(),True),\n",
    "     StructField(\"id\", IntegerType(),True),\n",
    "     StructField(\"nome\", StringType(),True),\n",
    "     StructField(\"idade\", IntegerType(),True),\n",
    "     StructField(\"sexo\", IntegerType(),True),\n",
    "     StructField(\"peso\", DoubleType(),True),\n",
    "     StructField(\"bpm\", DoubleType(),True),\n",
    "     StructField(\"pressao\", DoubleType(),True),\n",
    "     StructField(\"respiracao\", DoubleType(),True),\n",
    "     StructField(\"temperatura\", DoubleType(),True),\n",
    "     StructField(\"glicemia\", DoubleType(),True),\n",
    "     StructField(\"saturacao_oxigenio\", DoubleType(),True),\n",
    "     StructField(\"estado_atividade\", IntegerType(),True),\n",
    "     StructField(\"dia_de_semana\", IntegerType(),True),\n",
    "     StructField(\"periodo_do_dia\", IntegerType(), True),\n",
    "     StructField(\"semana_do_mes\", IntegerType(),True),\n",
    "     StructField(\"estacao_do_ano\", IntegerType(),True),\n",
    "     StructField(\"passos\", IntegerType(),True),\n",
    "     StructField(\"calorias\", DoubleType(),True),\n",
    "     StructField(\"distancia\", DoubleType(),True),\n",
    "     StructField(\"tempo\", DoubleType(),True),\n",
    "     StructField(\"total_sleep_last_24\", DoubleType(),True),\n",
    "     StructField(\"deep_sleep_last_24\", DoubleType(),True),\n",
    "     StructField(\"light_sleep_last_24\", DoubleType(),True),\n",
    "     StructField(\"awake_last_24\", DoubleType(),True),\n",
    "     StructField(\"timestampstr\", TimestampType(),True)\n",
    "    ])\n",
    "\n",
    "# Create dataframe setting schema for event data\n",
    "df_csv = (df1\n",
    "           # Sets schema for event data\n",
    "           .withColumn(\"value\", from_json(\"value\", schema_csv))\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f2ba14ba6a0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print to console\n",
    "df_csv.writeStream \\\n",
    "   .format(\"console\") \\\n",
    "   .outputMode(\"append\") \\\n",
    "   .start() \\\n",
    "   #.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_unixtime, to_date, to_timestamp, unix_timestamp\n",
    "\n",
    "# Transform into tabular \n",
    "# Convert unix timestamp to timestamp\n",
    "# Create partition column (change_timestamp_date)\n",
    "df_csv_formatted = (df_csv.select(\n",
    "    col(\"key\").alias(\"event_key\")\n",
    "    ,col(\"topic\").alias(\"event_topic\")\n",
    "    ,col(\"timestamp\").alias(\"event_timestamp\") \n",
    "    ,col(\"value.$schema\").alias(\"schema\")\n",
    "    ,col(\"value.id\").alias(\"id\")    \n",
    "    ,col(\"value.nome\").alias(\"nome\")    \n",
    "    ,col(\"value.idade\").alias(\"idade\")    \n",
    "    ,col(\"value.sexo\").alias(\"sexo\")    \n",
    "    ,col(\"value.bpm\").alias(\"bpm\")        \n",
    "    ,col(\"value.pressao\").alias(\"pressao\")        \n",
    "    ,col(\"value.respiracao\").alias(\"respiracao\")        \n",
    "    ,col(\"value.temperatura\").alias(\"temperatura\")        \n",
    "    ,col(\"value.glicemia\").alias(\"glicemia\")        \n",
    "    ,col(\"value.saturacao_oxigenio\").alias(\"saturacao_oxigenio\")        \n",
    "    ,col(\"value.estado_atividade\").alias(\"estado_atividade\")        \n",
    "    ,col(\"value.dia_de_semana\").alias(\"dia_de_semana\")        \n",
    "    ,col(\"value.periodo_do_dia\").alias(\"periodo_do_dia\")        \n",
    "    ,col(\"value.semana_do_mes\").alias(\"semana_do_mes\")        \n",
    "    ,col(\"value.estacao_do_ano\").alias(\"estacao_do_ano\")        \n",
    "    ,col(\"value.passos\").alias(\"passos\")        \n",
    "    ,col(\"value.calorias\").alias(\"calorias\")        \n",
    "    ,col(\"value.distancia\").alias(\"distancia\")        \n",
    "    ,col(\"value.total_sleep_last_24\").alias(\"total_sleep_last_24\")        \n",
    "    ,col(\"value.deep_sleep_last_24\").alias(\"deep_sleep_last_24\")            \n",
    "    ,col(\"value.light_sleep_last_24\").alias(\"light_sleep_last_24\")        \n",
    "    ,col(\"value.awake_last_24\").alias(\"awake_last_24\")    \n",
    "    ,to_timestamp(unix_timestamp(col(\"value.timestampstr\"))).alias(\"change_timestamp\")\n",
    "    ,to_date(col(\"value.timestampstr\")).alias(\"change_timestamp_date\")\n",
    "    ,col(\"value.timestampstr\").alias(\"change_timestamp_str\")         \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f2c3c259190>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print to console\n",
    "df_csv_formatted.writeStream \\\n",
    "   .format(\"console\") \\\n",
    "   .outputMode(\"append\") \\\n",
    "   .start() \\\n",
    "   #.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start query stream over stream dataframe\n",
    "raw_path = \"/home/jovyan/work/data-lake/csv-changes\"\n",
    "checkpoint_path = \"/home/jovyan/work/data-lake/csv-changes-checkpoint\"\n",
    "\n",
    "queryStream =(\n",
    "    df_csv_formatted \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .queryName(\"csv_changes_ingestion-9\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .option(\"path\", raw_path) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .partitionBy(\"change_timestamp_date\", \"nome\") \\\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parquet files as stream to output the number of rows\n",
    "df_csv_changes = (\n",
    "    spark \\\n",
    "    .readStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .schema(df_csv_formatted.schema) \\\n",
    "    .load(raw_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to memory to count rows\n",
    "queryStreamMem = (df_csv_changes\n",
    " .writeStream\n",
    " .format(\"memory\")\n",
    " .queryName(\"csv_changes_count3\")\n",
    " .outputMode(\"update\")\n",
    " .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:1\n",
      "+---+-------+---------+-----------------+------------------+-----------------+------------------+------------+-------+---------------+-------------+-------+-----------+\n",
      "| id|   nome|registros|          avg_bpm|       avg_pressao|  avg_temperatura|    avg_respiracao|count_passos|max_bpm|max_temperatura|max_repiracao|min_bpm|min_pressao|\n",
      "+---+-------+---------+-----------------+------------------+-----------------+------------------+------------+-------+---------------+-------------+-------+-----------+\n",
      "|  3|antonio|      148|            75.75|116.20945945945945|36.74324324324324|15.993243243243244|         148|  124.0|           42.0|         20.0|   40.0|       63.0|\n",
      "|  1|   joao|      142|78.85915492957747|114.07042253521126|35.95774647887324|15.507042253521126|         142|  122.0|           42.0|         20.0|   40.0|       60.0|\n",
      "|  2|  maria|      110|78.81818181818181|119.64545454545454|36.24545454545454|15.836363636363636|         110|  128.0|           42.0|         20.0|   42.0|       60.0|\n",
      "+---+-------+---------+-----------------+------------------+-----------------+------------------+------------+-------+---------------+-------------+-------+-----------+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "'writeStream' can be called only on streaming Dataset/DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b09962b3ff56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0msql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select id, nome, count(*) as registros, avg(bpm) as avg_bpm, avg(pressao) as avg_pressao, avg(temperatura) as avg_temperatura, avg(respiracao) as avg_respiracao, count(passos) as count_passos, max(bpm) as max_bpm, max(temperatura) as max_temperatura, max(respiracao) as max_repiracao, min(bpm) as min_bpm, min(pressao) as min_pressao from csv_changes_count3 where id is not null group by id, nome order by nome\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CAST(id AS STRING) AS key\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_json(struct(*)) AS value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwriteStream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataStreamWriter\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \"\"\"\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataStreamWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjsq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'writeStream' can be called only on streaming Dataset/DataFrame"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Count rows every 5 seconds while stream is active\n",
    "try:\n",
    "    i=1\n",
    "    # While stream is active, print count\n",
    "    while len(spark.streams.active) > 0:\n",
    "        \n",
    "        # Clear output\n",
    "        clear_output(wait=True)\n",
    "        print(\"Run:{}\".format(i))\n",
    "        \n",
    "        lst_queries = []\n",
    "        for s in spark.streams.active:\n",
    "            lst_queries.append(s.name)\n",
    "\n",
    "        # Verify if wiki_changes_count query is active before count\n",
    "        if \"csv_changes_count3\" in lst_queries:\n",
    "            # Count number of events\n",
    "            # spark.sql(\"select count(1) as qty from csv_changes_count\").show()\n",
    "            sql = spark.sql(\"select id, nome, count(*) as registros, avg(bpm) as avg_bpm, avg(pressao) as avg_pressao, avg(temperatura) as avg_temperatura, avg(respiracao) as avg_respiracao, count(passos) as count_passos, max(bpm) as max_bpm, max(temperatura) as max_temperatura, max(respiracao) as max_repiracao, min(bpm) as min_bpm, min(pressao) as min_pressao from csv_changes_count3 where id is not null group by id, nome order by nome\")\n",
    "            sql.show()\n",
    "            sql.selectExpr(\"CAST(id AS STRING) AS key\", \"to_json(struct(*)) AS value\") \\\n",
    "            .writeStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"topic\", 'aggregated_data') \\\n",
    "            .option(\"kafka.bootstrap.servers\", \"kafka-server:29092\") \\\n",
    "            .option(\"checkpointLocation\", \"/home/jovyan/work/csv\") \\\n",
    "            .start()\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print(\"'csv_changes_count' query not found.\")\n",
    "\n",
    "        sleep(5)\n",
    "        i=i+1\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    # Stop Query Stream\n",
    "    queryStreamMem.stop()\n",
    "    \n",
    "    print(\"stream process interrupted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:5d3ccdf1-2085-4b7a-898f-03c892e0fc8b | NAME:csv_changes_ingestion-8\n",
      "ID:e02f35af-9206-420f-8d60-475131b0d39b | NAME:None\n"
     ]
    }
   ],
   "source": [
    "# Check active streams\n",
    "for s in spark.streams.active:\n",
    "    print(\"ID:{} | NAME:{}\".format(s.id, s.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop ingestion\n",
    "queryStream.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
