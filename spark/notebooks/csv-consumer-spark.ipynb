{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import random\n",
    "from confluent_kafka import Producer\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('csv-changes-event-consumer')\n",
    "         # Add kafka package\n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stream dataframe setting kafka server, topic and offset option\n",
    "df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka-server:29092\") # kafka server ip address inspect - something like 172.23.0.5\n",
    "  .option(\"subscribe\", \"national-health-indicators\") # topic\n",
    "  .option(\"startingOffsets\", \"earliest\") # start from beginning \n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read a small batch of data from kafka and display to the console\n",
    "\n",
    "mySchema = StructType([\n",
    " StructField(\"id\", IntegerType()),\n",
    " StructField(\"nome\", StringType()),\n",
    " StructField(\"idade\", IntegerType()),\n",
    " StructField(\"sexo\", IntegerType()),\n",
    " StructField(\"peso\", DoubleType()),\n",
    " StructField(\"bpm\", DoubleType()),\n",
    " StructField(\"pressao\", DoubleType()),\n",
    " StructField(\"respiracao\", DoubleType()),\n",
    " StructField(\"temperatura\", DoubleType()),\n",
    " StructField(\"glicemia\", DoubleType()),\n",
    " StructField(\"saturacao_oxigenio\", DoubleType()),\n",
    " StructField(\"estado_atividade\", IntegerType()),\n",
    " StructField(\"dia_de_semana\", IntegerType()),\n",
    " StructField(\"periodo_do_dia\", IntegerType()),\n",
    " StructField(\"semana_do_mes\", IntegerType()),\n",
    " StructField(\"estacao_do_ano\", IntegerType()),\n",
    " StructField(\"passos\", IntegerType()),\n",
    " StructField(\"calorias\", DoubleType()),\n",
    " StructField(\"distancia\", DoubleType()),\n",
    " StructField(\"tempo\", DoubleType()),\n",
    " StructField(\"total_sleep_last_24\", DoubleType()),\n",
    " StructField(\"deep_sleep_last_24\", DoubleType()),\n",
    " StructField(\"light_sleep_last_24\", DoubleType()),\n",
    " StructField(\"awake_last_24\", DoubleType()),\n",
    " StructField(\"timestampstr\", TimestampType())\n",
    " \n",
    "])\n",
    "\n",
    "df_json.select(from_json(df_json.json, mySchema).alias('raw_data')) \\\n",
    "  .select('raw_data.*') \\\n",
    "  .writeStream \\\n",
    "  .trigger(once=True) \\\n",
    "  .format(\"console\") \\\n",
    "  .start() \\\n",
    "  .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Can't extract value from value#8: need struct type but got binary",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-7b29243aec95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CAST(value[\"nome\"] AS STRING) as nome'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselectExpr\u001b[0;34m(self, *expr)\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m             \u001b[0mexpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1686\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1687\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Can't extract value from value#8: need struct type but got binary"
     ]
    }
   ],
   "source": [
    "#df_json = df.selectExpr('CAST(value AS STRING) as nome')\n",
    "df_json = df.selectExpr(\"array(struct(items[0].value as foo, items[1].value as bar)) as items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[nome: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ml predict\n",
    "df_json = df.selectExpr('CAST(value.nome AS STRING) as nome')\n",
    "\n",
    "messageParsedDF = df_json.select(from_json(\"json\", mySchema).alias(\"message\"))\n",
    "\n",
    "messageFlattenedDF = messageParsedDF.selectExpr(\"message.nome\")\n",
    "\n",
    "messageFlattenedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-322affaaf636>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# alternative - print to console\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmessageFlattenedDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"console\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# alternative - print to console\n",
    "messageFlattenedDF.writeStream \\\n",
    "   .format(\"console\") \\\n",
    "   .outputMode(\"append\") \\\n",
    "   .start() \\\n",
    "   .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'nome'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messageFlattenedDF['nome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml predict\n",
    "def apply_sentiment_analysis(data):\n",
    "    import requests\n",
    "    import json\n",
    "        \n",
    "    result = requests.post('http://localhost:4000/predict', json=json.loads(data))\n",
    "    return json.dumps(result.json())\n",
    "\n",
    "vader_udf = udf(lambda data: apply_sentiment_analysis(data), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'json'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "Writing job aborted.\n=== Streaming Query ===\nIdentifier: [id = c14d03e5-f95e-4ad3-a3c7-9fa3f5b0b357, runId = acb5b0f4-95e7-4356-9db6-f963fd5e1353]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {KafkaV2[Subscribe[national-health-indicators]]: {\"national-health-indicators\":{\"0\":301}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource ConsoleWriter[numRows=20, truncate=true]\n+- Project [sentence#949, response#951.neg AS neg#954, response#951.pos AS pos#955, response#951.neu AS neu#956, response#951.compound AS compound#957]\n   +- Project [from_json(StructField(nome,StringType,true), json#822, Some(Etc/UTC)) AS sentence#949, from_json(StructField(neg,StringType,true), StructField(pos,StringType,true), StructField(neu,StringType,true), StructField(compound,StringType,true), <lambda>(json#822), Some(Etc/UTC)) AS response#951]\n      +- Project [cast(value#8 as string) AS json#822]\n         +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@4ec8cd7, KafkaV2[Subscribe[national-health-indicators]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-01901b11d76e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             StructField('compound', StringType())])\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m df_json.select(from_json(df_json.json, schema_input).alias('sentence'),\\\n\u001b[0m\u001b[1;32m      8\u001b[0m                from_json(vader_udf(df_json.json), schema_output).alias('response'))\\\n\u001b[1;32m      9\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'response.*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: Writing job aborted.\n=== Streaming Query ===\nIdentifier: [id = c14d03e5-f95e-4ad3-a3c7-9fa3f5b0b357, runId = acb5b0f4-95e7-4356-9db6-f963fd5e1353]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {KafkaV2[Subscribe[national-health-indicators]]: {\"national-health-indicators\":{\"0\":301}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource ConsoleWriter[numRows=20, truncate=true]\n+- Project [sentence#949, response#951.neg AS neg#954, response#951.pos AS pos#955, response#951.neu AS neu#956, response#951.compound AS compound#957]\n   +- Project [from_json(StructField(nome,StringType,true), json#822, Some(Etc/UTC)) AS sentence#949, from_json(StructField(neg,StringType,true), StructField(pos,StringType,true), StructField(neu,StringType,true), StructField(compound,StringType,true), <lambda>(json#822), Some(Etc/UTC)) AS response#951]\n      +- Project [cast(value#8 as string) AS json#822]\n         +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@4ec8cd7, KafkaV2[Subscribe[national-health-indicators]]\n"
     ]
    }
   ],
   "source": [
    "schema_input = StructType([StructField('nome', StringType())])\n",
    "schema_output = StructType([StructField('neg', StringType()),\\\n",
    "                            StructField('pos', StringType()),\\\n",
    "                            StructField('neu', StringType()),\\\n",
    "                            StructField('compound', StringType())])\n",
    "\n",
    "df_json.select(from_json(df_json.json, schema_input).alias('sentence'),\\\n",
    "               from_json(vader_udf(df_json.json), schema_output).alias('response'))\\\n",
    "  .select('sentence', 'response.*') \\\n",
    "  .writeStream \\\n",
    "  .trigger(once=True) \\\n",
    "  .format(\"console\") \\\n",
    "  .start() \\\n",
    "  .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Convert binary to string key and value\n",
    "df1 = (df\n",
    "    .withColumn(\"key\", df[\"key\"].cast(StringType()))\n",
    "    .withColumn(\"value\", df[\"value\"].cast(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print to jupyter output in two fases - memory and display\n",
    "query = df1 \\\n",
    "    .withWatermark(\"timestampstr\", \"3 minutes\") \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"csv_query2\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "|key|               value|               topic|partition|offset|           timestamp|timestampType|\n",
      "+---+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "|  1|{\"id\":1,\"nome\":\"j...|national-health-i...|        0|     0|2022-03-20 00:22:...|            0|\n",
      "|  3|{\"id\":3,\"nome\":\"a...|national-health-i...|        0|     1|2022-03-20 00:22:...|            0|\n",
      "|  3|{\"id\":3,\"nome\":\"a...|national-health-i...|        0|     2|2022-03-20 00:22:...|            0|\n",
      "|  2|{\"id\":2,\"nome\":\"m...|national-health-i...|        0|     3|2022-03-20 00:22:...|            0|\n",
      "|  3|{\"id\":3,\"nome\":\"a...|national-health-i...|        0|     4|2022-03-20 00:22:...|            0|\n",
      "|  1|{\"id\":1,\"nome\":\"j...|national-health-i...|        0|     5|2022-03-20 00:22:...|            0|\n",
      "|  1|{\"id\":1,\"nome\":\"j...|national-health-i...|        0|     6|2022-03-20 00:22:...|            0|\n",
      "|  1|{\"id\":1,\"nome\":\"j...|national-health-i...|        0|     7|2022-03-20 00:22:...|            0|\n",
      "|  2|{\"id\":2,\"nome\":\"m...|national-health-i...|        0|     8|2022-03-20 00:22:...|            0|\n",
      "|  3|{\"id\":3,\"nome\":\"a...|national-health-i...|        0|     9|2022-03-20 00:22:...|            0|\n",
      "|  3|{\"id\":3,\"nome\":\"a...|national-health-i...|        0|    10|2022-03-20 00:22:...|            0|\n",
      "|  2|{\"id\":2,\"nome\":\"m...|national-health-i...|        0|    11|2022-03-20 00:22:...|            0|\n",
      "|  2|{\"id\":2,\"nome\":\"m...|national-health-i...|        0|    12|2022-03-20 00:22:...|            0|\n",
      "|  3|{\"id\":3,\"nome\":\"a...|national-health-i...|        0|    13|2022-03-20 00:22:...|            0|\n",
      "|  1|{\"id\":1,\"nome\":\"j...|national-health-i...|        0|    14|2022-03-20 00:22:...|            0|\n",
      "|  2|{\"id\":2,\"nome\":\"m...|national-health-i...|        0|    15|2022-03-20 00:22:...|            0|\n",
      "|  2|{\"id\":2,\"nome\":\"m...|national-health-i...|        0|    16|2022-03-20 00:22:...|            0|\n",
      "|  3|{\"id\":3,\"nome\":\"a...|national-health-i...|        0|    17|2022-03-20 00:22:...|            0|\n",
      "|  3|{\"id\":3,\"nome\":\"a...|national-health-i...|        0|    18|2022-03-20 00:22:...|            0|\n",
      "|  2|{\"id\":2,\"nome\":\"m...|national-health-i...|        0|    19|2022-03-20 00:22:...|            0|\n",
      "+---+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a579930eb018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SELECT * FROM csv_query'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m199\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# display the output\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(query.status)\n",
    "    display(spark.sql('SELECT * FROM csv_query').show())\n",
    "    time.sleep(199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative - print to console\n",
    "df1.writeStream \\\n",
    "   .format(\"console\") \\\n",
    "   .outputMode(\"append\") \\\n",
    "   .start() \\\n",
    "   .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, LongType, IntegerType, DoubleType\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Event data schema\n",
    "schema_csv = StructType(\n",
    "    [StructField(\"$schema\",StringType(),True),\n",
    "     StructField(\"id\", IntegerType(),True),\n",
    "     StructField(\"nome\", StringType(),True),\n",
    "     StructField(\"idade\", IntegerType(),True),\n",
    "     StructField(\"sexo\", IntegerType(),True),\n",
    "     StructField(\"peso\", DoubleType(),True),\n",
    "     StructField(\"bpm\", DoubleType(),True),\n",
    "     StructField(\"pressao\", DoubleType(),True),\n",
    "     StructField(\"respiracao\", DoubleType(),True),\n",
    "     StructField(\"temperatura\", DoubleType(),True),\n",
    "     StructField(\"glicemia\", DoubleType(),True),\n",
    "     StructField(\"saturacao_oxigenio\", DoubleType(),True),\n",
    "     StructField(\"estado_atividade\", IntegerType(),True),\n",
    "     StructField(\"dia_de_semana\", IntegerType(),True),\n",
    "     StructField(\"periodo_do_dia\", IntegerType(), True),\n",
    "     StructField(\"semana_do_mes\", IntegerType(),True),\n",
    "     StructField(\"estacao_do_ano\", IntegerType(),True),\n",
    "     StructField(\"passos\", IntegerType(),True),\n",
    "     StructField(\"calorias\", DoubleType(),True),\n",
    "     StructField(\"distancia\", DoubleType(),True),\n",
    "     StructField(\"tempo\", DoubleType(),True),\n",
    "     StructField(\"total_sleep_last_24\", DoubleType(),True),\n",
    "     StructField(\"deep_sleep_last_24\", DoubleType(),True),\n",
    "     StructField(\"light_sleep_last_24\", DoubleType(),True),\n",
    "     StructField(\"awake_last_24\", DoubleType(),True),\n",
    "     StructField(\"timestampstr\", TimestampType(),True)\n",
    "    ])\n",
    "\n",
    "# Create dataframe setting schema for event data\n",
    "df_csv = (df1\n",
    "           # Sets schema for event data\n",
    "           .withColumn(\"value\", from_json(\"value\", schema_csv))\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f5fe523c400>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print to console\n",
    "df_csv.writeStream \\\n",
    "   .format(\"console\") \\\n",
    "   .outputMode(\"append\") \\\n",
    "   .start() \\\n",
    "   #.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_unixtime, to_date, to_timestamp, unix_timestamp\n",
    "\n",
    "# Transform into tabular \n",
    "# Convert unix timestamp to timestamp\n",
    "# Create partition column (change_timestamp_date)\n",
    "df_csv_formatted = (df_csv.select(\n",
    "    col(\"key\").alias(\"event_key\")\n",
    "    ,col(\"topic\").alias(\"event_topic\")\n",
    "    ,col(\"timestamp\").alias(\"event_timestamp\") \n",
    "    ,col(\"value.$schema\").alias(\"schema\")\n",
    "    ,col(\"value.id\").alias(\"id\")    \n",
    "    ,col(\"value.nome\").alias(\"nome\")    \n",
    "    ,col(\"value.idade\").alias(\"idade\")    \n",
    "    ,col(\"value.sexo\").alias(\"sexo\")    \n",
    "    ,col(\"value.bpm\").alias(\"bpm\")        \n",
    "    ,col(\"value.pressao\").alias(\"pressao\")        \n",
    "    ,col(\"value.respiracao\").alias(\"respiracao\")        \n",
    "    ,col(\"value.temperatura\").alias(\"temperatura\")        \n",
    "    ,col(\"value.glicemia\").alias(\"glicemia\")        \n",
    "    ,col(\"value.saturacao_oxigenio\").alias(\"saturacao_oxigenio\")        \n",
    "    ,col(\"value.estado_atividade\").alias(\"estado_atividade\")        \n",
    "    ,col(\"value.dia_de_semana\").alias(\"dia_de_semana\")        \n",
    "    ,col(\"value.periodo_do_dia\").alias(\"periodo_do_dia\")        \n",
    "    ,col(\"value.semana_do_mes\").alias(\"semana_do_mes\")        \n",
    "    ,col(\"value.estacao_do_ano\").alias(\"estacao_do_ano\")        \n",
    "    ,col(\"value.passos\").alias(\"passos\")        \n",
    "    ,col(\"value.calorias\").alias(\"calorias\")        \n",
    "    ,col(\"value.distancia\").alias(\"distancia\")        \n",
    "    ,col(\"value.total_sleep_last_24\").alias(\"total_sleep_last_24\")        \n",
    "    ,col(\"value.deep_sleep_last_24\").alias(\"deep_sleep_last_24\")            \n",
    "    ,col(\"value.light_sleep_last_24\").alias(\"light_sleep_last_24\")        \n",
    "    ,col(\"value.awake_last_24\").alias(\"awake_last_24\")    \n",
    "    ,to_timestamp(unix_timestamp(col(\"value.timestampstr\"))).alias(\"change_timestamp\")\n",
    "    ,to_date(col(\"value.timestampstr\")).alias(\"change_timestamp_date\")\n",
    "    ,col(\"value.timestampstr\").alias(\"change_timestamp_str\")         \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f4a75e065b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print to console\n",
    "df_csv_formatted.writeStream \\\n",
    "   .format(\"console\") \\\n",
    "   .outputMode(\"append\") \\\n",
    "   .start() \\\n",
    "   #.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start query stream over stream dataframe\n",
    "raw_path = \"/home/jovyan/work/data-lake/csv-changes\"\n",
    "checkpoint_path = \"/home/jovyan/work/data-lake/csv-changes-checkpoint\"\n",
    "\n",
    "queryStream =(\n",
    "    df_csv_formatted \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .queryName(\"csv_changes_ingestion-9\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .option(\"path\", raw_path) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .partitionBy(\"change_timestamp_date\", \"nome\") \\\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parquet files as stream to output the number of rows\n",
    "df_csv_changes = (\n",
    "    spark \\\n",
    "    .readStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .schema(df_csv_formatted.schema) \\\n",
    "    .load(raw_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to memory to count rows\n",
    "queryStreamMem = (df_csv_changes\n",
    " .writeStream\n",
    " .format(\"memory\")\n",
    " .queryName(\"csv_changes_count\")\n",
    " .outputMode(\"update\")\n",
    " .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:68\n",
      "+------+\n",
      "|   bpm|\n",
      "+------+\n",
      "|79.085|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Count rows every 5 seconds while stream is active\n",
    "try:\n",
    "    i=1\n",
    "    # While stream is active, print count\n",
    "    while len(spark.streams.active) > 0:\n",
    "        \n",
    "        # Clear output\n",
    "        clear_output(wait=True)\n",
    "        print(\"Run:{}\".format(i))\n",
    "        \n",
    "        lst_queries = []\n",
    "        for s in spark.streams.active:\n",
    "            lst_queries.append(s.name)\n",
    "\n",
    "        # Verify if wiki_changes_count query is active before count\n",
    "        if \"csv_changes_count\" in lst_queries:\n",
    "            # Count number of events\n",
    "            # spark.sql(\"select count(1) as qty from csv_changes_count\").show()\n",
    "            spark.sql(\"select avg(bpm) as bpm from csv_changes_count\").show()\n",
    "            \n",
    "        else:\n",
    "            print(\"'csv_changes_count' query not found.\")\n",
    "\n",
    "        sleep(5)\n",
    "        i=i+1\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    # Stop Query Stream\n",
    "    queryStreamMem.stop()\n",
    "    \n",
    "    print(\"stream process interrupted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:5d3ccdf1-2085-4b7a-898f-03c892e0fc8b | NAME:csv_changes_ingestion-8\n",
      "ID:e02f35af-9206-420f-8d60-475131b0d39b | NAME:None\n"
     ]
    }
   ],
   "source": [
    "# Check active streams\n",
    "for s in spark.streams.active:\n",
    "    print(\"ID:{} | NAME:{}\".format(s.id, s.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop ingestion\n",
    "queryStream.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
